<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Excel Data Matcher - Web App</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css">
    <style>
        body { 
            padding: 20px; 
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
        }
        .container { 
            max-width: 1200px; 
            background: white;
            border-radius: 15px;
            padding: 30px;
            box-shadow: 0 10px 40px rgba(0,0,0,0.2);
        }
        .file-input-group {
            margin: 20px 0;
            padding: 20px;
            border: 2px dashed #667eea;
            border-radius: 10px;
            background: #f8f9fa;
        }
        #output {
            background: #1e1e1e;
            color: #d4d4d4;
            padding: 20px;
            border-radius: 8px;
            font-family: 'Courier New', monospace;
            font-size: 13px;
            max-height: 600px;
            overflow-y: auto;
            white-space: pre-wrap;
            word-wrap: break-word;
        }
        .success { color: #4ade80; }
        .error { color: #f87171; }
        .info { color: #60a5fa; }
        .warning { color: #fbbf24; }
        .btn-primary {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            border: none;
            padding: 12px 30px;
            font-weight: bold;
        }
        .btn-primary:hover {
            transform: translateY(-2px);
            box-shadow: 0 5px 15px rgba(102, 126, 234, 0.4);
        }
        #loading {
            display: none;
            text-align: center;
            margin: 20px 0;
        }
        .spinner-border {
            width: 3rem;
            height: 3rem;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1 class="text-center mb-4">üî¨ Excel Data Matcher</h1>
        <p class="text-center text-muted">Semiconductor Manufacturing Data Analysis Tool v4.2</p>
        
        <div class="file-input-group">
            <label class="form-label"><strong>üìä Summary File (CSV/Excel)</strong></label>
            <input type="file" class="form-control" id="summaryFile" accept=".csv,.xlsx,.xls" required>
            <small class="text-muted">Upload your CatBoost predictions file containing True_Hardness column</small>
        </div>

        <div class="file-input-group">
            <label class="form-label"><strong>üìÅ Data Files (CSV/Excel - Multiple)</strong></label>
            <input type="file" class="form-control" id="dataFiles" accept=".csv,.xlsx,.xls" multiple required>
            <small class="text-muted">Upload files containing H(GPa) and Er(GPa) columns</small>
        </div>

        <div class="row mb-4">
            <div class="col-md-6">
                <label class="form-label">Divisor (True_Hardness √∑ ?)</label>
                <input type="number" class="form-control" id="divisor" value="102" step="0.1">
            </div>
            <div class="col-md-6">
                <label class="form-label">Tolerance (%)</label>
                <input type="number" class="form-control" id="tolerance" value="0.05" step="0.01">
            </div>
        </div>

        <div class="text-center">
            <button class="btn btn-primary btn-lg" onclick="processFiles()">
                üöÄ Process & Match Data
            </button>
        </div>

        <div id="loading">
            <div class="spinner-border text-primary" role="status"></div>
            <p class="mt-2">Processing... Please wait</p>
        </div>

        <div id="output" class="mt-4"></div>
    </div>

    <script src="https://cdn.jsdelivr.net/pyodide/v0.24.1/full/pyodide.js"></script>
    <script>
        let pyodide;
        let outputDiv = document.getElementById('output');

        function log(message, type = 'info') {
            const span = document.createElement('span');
            span.className = type;
            span.textContent = message + '\n';
            outputDiv.appendChild(span);
            outputDiv.scrollTop = outputDiv.scrollHeight;
        }

        async function initPyodide() {
            if (pyodide) return;
            
            log('üîß Initializing Python environment...', 'info');
            pyodide = await loadPyodide();
            
            log('üì¶ Installing dependencies (pandas, openpyxl)...', 'info');
            await pyodide.loadPackage(['pandas', 'micropip']);
            await pyodide.runPythonAsync(`
                import micropip
                await micropip.install(['openpyxl', 'numpy'])
            `);
            
            log('‚úÖ Python environment ready!', 'success');
        }

        async function processFiles() {
            const summaryFile = document.getElementById('summaryFile').files[0];
            const dataFiles = document.getElementById('dataFiles').files;
            const divisor = parseFloat(document.getElementById('divisor').value);
            const tolerance = parseFloat(document.getElementById('tolerance').value);

            if (!summaryFile || dataFiles.length === 0) {
                alert('Please upload both summary file and data files!');
                return;
            }

            outputDiv.innerHTML = '';
            document.getElementById('loading').style.display = 'block';

            try {
                await initPyodide();

                // Upload summary file
                log(`üì§ Uploading summary file: ${summaryFile.name}`, 'info');
                const summaryData = await summaryFile.arrayBuffer();
                pyodide.FS.writeFile('summary_file', new Uint8Array(summaryData));

                // Upload data files
                const dataFilenames = [];
                for (let i = 0; i < dataFiles.length; i++) {
                    const file = dataFiles[i];
                    log(`üì§ Uploading data file ${i+1}: ${file.name}`, 'info');
                    const data = await file.arrayBuffer();
                    const filename = `data_file_${i}`;
                    pyodide.FS.writeFile(filename, new Uint8Array(data));
                    dataFilenames.push(filename);
                }

                log('\n' + '='.repeat(80), 'info');
                log('üî• STARTING DATA MATCHING PROCESS', 'success');
                log('='.repeat(80) + '\n', 'info');

                // **FIX: Actually call the matching logic!**
                const result = await pyodide.runPythonAsync(`
import pandas as pd
import numpy as np
import io
import re
import warnings
from pathlib import Path
warnings.filterwarnings('ignore')

# ===== EMBED YOUR CORE LOGIC HERE =====
${getCorePythonCode()}

# ===== MAIN EXECUTION =====
def web_main():
    print("=" * 80)
    print("  ENTERPRISE DATA MATCHER V4.2 - WEB VERSION")
    print("  True_Hardness √∑ ${divisor} ‚Üí Match H(GPa) ‚Üí Extract Er(GPa)")
    print("=" * 80)
    print()
    
    # Configuration
    summary_file = 'summary_file'
    data_files = ${JSON.stringify(dataFilenames)}
    
    matcher = ExcelDataMatcher(
        summary_file=summary_file,
        data_files=data_files,
        match_column='H(GPa)',
        extract_column='Er(GPa)',
        summary_source_column='True_Hardness',
        divisor=${divisor},
        tolerance_percent=${tolerance},
        additional_columns=['Contact_Depth', 'Load'],
        best_match_only=True
    )
    
    # Execute pipeline
    matcher.load_data(parallel=False)
    result_df = matcher.match_data()
    
    # Save to memory
    output_bytes = io.BytesIO()
    with pd.ExcelWriter(output_bytes, engine='openpyxl') as writer:
        result_df.to_excel(writer, sheet_name='Full_Data', index=False)
        summary_df = matcher.create_summary_sheet(result_df)
        if not summary_df.empty:
            summary_df.to_excel(writer, sheet_name='Summary', index=False)
    
    output_bytes.seek(0)
    return output_bytes.getvalue()

# Run and return result
result = web_main()
result
                `);

                log('\n‚úÖ Processing complete!', 'success');
                log('üì• Downloading result file...', 'info');

                // Download the result
                const blob = new Blob([result.toJs()], {
                    type: 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
                });
                const url = URL.createObjectURL(blob);
                const a = document.createElement('a');
                a.href = url;
                a.download = `matched_results_${new Date().getTime()}.xlsx`;
                a.click();
                URL.revokeObjectURL(url);

                log('‚úÖ Download initiated!', 'success');
                log('üìä Check the "Summary" tab for essential columns', 'info');

            } catch (error) {
                log(`\n‚ùå Error: ${error.message}`, 'error');
                console.error(error);
            } finally {
                document.getElementById('loading').style.display = 'none';
            }
        }

        function getCorePythonCode() {
            // Return the complete core logic as a string
            return `
def normalize_column_name(col_name):
    if pd.isna(col_name):
        return ""
    normalized = str(col_name).strip().lower()
    normalized = re.sub(r'\\s+', ' ', normalized)
    return normalized

def find_column_fuzzy(df, target_names, show_available=False):
    if show_available:
        print(f"  Available columns: {list(df.columns)}")
    
    normalized_targets = [normalize_column_name(name) for name in target_names]
    
    for col in df.columns:
        if col in target_names:
            return col
    
    for col in df.columns:
        normalized_col = normalize_column_name(col)
        if normalized_col in normalized_targets:
            return col
    
    for col in df.columns:
        normalized_col = normalize_column_name(col)
        for target in normalized_targets:
            if target in normalized_col or normalized_col in target:
                return col
    
    return None

def detect_header_row(file_path, max_rows_to_check=10):
    file_extension = Path(file_path).suffix.lower()
    
    if file_extension == '.csv' or not file_extension:
        preview = pd.read_csv(file_path, nrows=max_rows_to_check, header=None)
    else:
        preview = pd.read_excel(file_path, nrows=max_rows_to_check, header=None, engine='openpyxl')
    
    best_header_row = 0
    max_text_count = 0
    
    for idx in range(min(max_rows_to_check, len(preview))):
        row = preview.iloc[idx]
        text_count = sum(1 for val in row if pd.notna(val) and 
                        not (isinstance(val, (int, float)) and not isinstance(val, bool)))
        if text_count > max_text_count:
            max_text_count = text_count
            best_header_row = idx
    
    if file_extension == '.csv' or not file_extension:
        df = pd.read_csv(file_path, header=best_header_row)
    else:
        df = pd.read_excel(file_path, header=best_header_row, engine='openpyxl')
    
    df.columns = [str(col).strip() if pd.notna(col) else f"Unnamed_{i}" 
                  for i, col in enumerate(df.columns)]
    
    return df, best_header_row

def read_file_auto(file_path):
    try:
        df, header_row = detect_header_row(file_path)
        if header_row > 0:
            print(f"  ‚ÑπÔ∏è Header detected at row {header_row}")
        return df
    except Exception as e:
        print(f"  ‚ö†Ô∏è Error reading file: {e}")
        file_extension = Path(file_path).suffix.lower()
        if file_extension == '.csv' or not file_extension:
            return pd.read_csv(file_path)
        else:
            return pd.read_excel(file_path, engine='openpyxl')

class ExcelDataMatcher:
    def __init__(self, summary_file, data_files, match_column='H(GPa)', 
                 extract_column='Er(GPa)', summary_source_column='True_Hardness',
                 divisor=102.0, tolerance_percent=2.0, additional_columns=None,
                 best_match_only=True):
        self.summary_file = summary_file
        self.data_files = data_files
        self.match_column = match_column
        self.extract_column = extract_column
        self.summary_source_column = summary_source_column
        self.divisor = divisor
        self.tolerance_percent = tolerance_percent
        self.additional_columns = additional_columns or []
        self.best_match_only = best_match_only
        self.summary_df = None
        self.data_dfs = {}
        
        self.match_column_variants = [
            match_column, match_column.lower(), match_column.replace('(', ' ('),
            match_column.replace(')', ') '), 'h(gpa)', 'h (gpa)', 'H', 'H(GPa)'
        ]
        
        self.extract_column_variants = [
            extract_column, extract_column.lower(), extract_column.replace('(', ' ('),
            extract_column.replace(')', ') '), 'er(gpa)', 'er (gpa)', 'Er', 'Er(GPa)'
        ]
    
    def load_data(self, parallel=False):
        print(f"üìä Loading summary file...")
        self.summary_df = read_file_auto(self.summary_file)
        print(f"  Summary shape: {self.summary_df.shape}")
        
        summary_col = find_column_fuzzy(self.summary_df,
                                       [self.summary_source_column, 
                                        self.summary_source_column.lower(),
                                        'true_hardness'])
        
        if summary_col is None:
            raise ValueError(f"Column '{self.summary_source_column}' not found!")
        
        self.summary_source_column = summary_col
        self.summary_df[self.summary_source_column] = pd.to_numeric(
            self.summary_df[self.summary_source_column], errors='coerce')
        
        self.summary_df['_transformed_value'] = self.summary_df[self.summary_source_column] / self.divisor
        print(f"  ‚úì Transformed (√∑{self.divisor}): {self.summary_df['_transformed_value'].head(3).tolist()}")
        
        print(f"\\nüìÅ Loading {len(self.data_files)} data files...")
        for idx, file_path in enumerate(self.data_files, 1):
            file_key = f"file_{idx}"
            print(f"  {idx}. Loading...")
            try:
                df = read_file_auto(file_path)
                self.data_dfs[file_key] = df
                print(f"    Shape: {df.shape}")
            except Exception as e:
                print(f"    ‚ùå Error: {e}")
    
    def match_data(self):
        print(f"\\n‚ö° Matching data...")
        print(f"  Strategy: {self.summary_source_column} √∑ {self.divisor} ‚Üí {self.match_column} ‚Üí {self.extract_column}")
        print(f"  Tolerance: ¬±{self.tolerance_percent}%")
        
        all_matches = []
        consolidated_data = []
        
        for file_key, df in self.data_dfs.items():
            match_col = find_column_fuzzy(df, self.match_column_variants)
            if match_col is None:
                continue
            
            match_values = pd.to_numeric(df[match_col], errors='coerce')
            valid_mask = match_values.notna()
            
            extract_col = find_column_fuzzy(df, self.extract_column_variants)
            if extract_col:
                extract_values = pd.to_numeric(df[extract_col], errors='coerce')
            else:
                extract_values = pd.Series([None] * len(df))
            
            for idx in df.index[valid_mask]:
                entry = {
                    'file_source': file_key,
                    'match_value': match_values.iloc[idx],
                    'extract_value': extract_values.iloc[idx] if extract_col else None,
                }
                consolidated_data.append(entry)
        
        if not consolidated_data:
            print("\\n‚ùå No valid data found!")
            result_df = self.summary_df.copy()
            result_df['match_found'] = False
            return result_df
        
        consolidated_df = pd.DataFrame(consolidated_data)
        print(f"\\n  ‚úì Consolidated {len(consolidated_df)} data points")
        
        matched_count = 0
        for idx, target_value in enumerate(self.summary_df['_transformed_value'].values):
            if pd.isna(target_value):
                continue
            
            tolerance_abs = abs(target_value * self.tolerance_percent / 100.0)
            lower_bound = target_value - tolerance_abs
            upper_bound = target_value + tolerance_abs
            
            matches_mask = (consolidated_df['match_value'] >= lower_bound) & \\
                          (consolidated_df['match_value'] <= upper_bound)
            matches = consolidated_df[matches_mask].copy()
            
            if len(matches) == 0:
                continue
            
            matches['relative_error'] = abs(matches['match_value'] - target_value) / target_value * 100
            matches = matches.sort_values('relative_error')
            
            best_match = matches.iloc[0]
            match_record = self.summary_df.iloc[idx].to_dict()
            match_record['matched_source_file'] = best_match['file_source']
            match_record[f'matched_{self.match_column}_value'] = best_match['match_value']
            match_record[f'extracted_{self.extract_column}_value'] = best_match['extract_value']
            match_record['relative_error_percent'] = best_match['relative_error']
            match_record['match_found'] = True
            
            all_matches.append(match_record)
            matched_count += 1
        
        if all_matches:
            result_df = pd.DataFrame(all_matches)
        else:
            result_df = self.summary_df.copy()
            result_df['match_found'] = False
        
        match_rate = (matched_count / len(self.summary_df) * 100) if len(self.summary_df) > 0 else 0
        print(f"\\n‚úì Matched: {matched_count} ({match_rate:.1f}%)")
        
        return result_df
    
    def create_summary_sheet(self, result_df):
        print(f"\\nüìã Creating summary sheet...")
        
        column_mapping = {
            'X': 'X',
            'Y': 'Y',
            'Cluster': 'Cluster',
            self.summary_source_column: 'True_Hardness',
            'CatBoost_WithMicro': 'CatBoost_WithMicro',
            f'matched_{self.match_column}_value': 'H(GPa)',
            f'extracted_{self.extract_column}_value': 'Er(GPa)'
        }
        
        available_cols = []
        rename_dict = {}
        for orig_col, new_col in column_mapping.items():
            if orig_col in result_df.columns:
                available_cols.append(orig_col)
                rename_dict[orig_col] = new_col
        
        if not available_cols:
            return pd.DataFrame()
        
        summary_df = result_df[available_cols].copy()
        summary_df = summary_df.rename(columns=rename_dict)
        
        print(f"  ‚úì Summary sheet created with {len(summary_df)} rows")
        return summary_df
`;
        }
    </script>
</body>
</html>

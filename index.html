<!DOCTYPE html>
<html lang="en">
<head><script type='text/javascript' src='https://ppl-ai-code-interpreter-files.s3.amazonaws.com/PrVgjcYMMjkRWzGnsUsuy5X9StCUCpvt5Kfk46MLz2uRpLuqi9gV1VF09K5Tkd08ipSWfYiMv7l9lPgx4cSKKOYGtp8WiDVw286SdP7_E-mnTp_y9kOvgPp3TwCHOXPfHUoJT6rpNzZCguVuAIswk_xQidiEQP5d89LAeLF7ESNPKGMLE1n_21D7197fKXJthDSDHxkGtF4qDTuwQBYGSV-7-g14fcP1taFmMFnUojbnQchxq-ruHLwtEJgY7N8otkq1FW5XMZHrgy9zkCndRtRjWz3XQphHIZnn7VlEDmUk7xKDR_4qInF9JnfJSvwChNk2rlqBSqXAA6HssCwKr-4bXKZszyDV-2qizDGBwbK1zr0ung_FENz7gfA-059aoOJisbqUADja-EnDoPHmh_6uyd140jnbMuwOcOvWWg2oaNWnaCV6H3Dh2LbNK3BIlk13z1E1LS8ogS-HgZs4PL5BfT3bzSA1IRfOt4c_y5FYr4Xi4Gpvyo2jfxZowwgcFy8F6X8GN_t5hw6YiZoxeNdlL4N325qJ2zJkNGECDlY9DsW-EFShDuf-V7TRj4vXBi1pWRYUpnXs_wfpfhzGjLVX9RqFgn3jktPYZf7qPH3cfZiFB8yT39vuyA-noWR0vxZfvg54ZACBZdgiHJIQxpfpmVsXMbJhMYzxs9FcF8MlahxQnjBJi3L1J4Lxn8TYsmubMRcQ7GIOQ7Q4mfgtivyAHlGbhdw4xebkvfVyS2lfdCvelYoZD-4r-Q_59t6MCrIb8iKmInawZxoO8ZIyAT8J4f38PfitXFIXRimUhq9x7kdIpbRYnm1Gnpbu-XkLilM2r2zcUz2v1uxWHvo7j_vdQas9S78eVtqbStxlw017puZOgq_x613Iu8NEuzDRWGm8v_w19YR8d5MlfDyUuVKdsmyciVcuffGTy2W4TYFBByj5J-41xqNsvOMuVzpu1q7VkHs94Dxzzonn9XO2B3MxNTL67jxW3RWcn-qxXPK5mux_J-g0Fr6rIQlLL-qnYgykscTpNYTtSfFlMP_5a0Vb8NK1xO2LwJ6BzZpWL2LjXdj4jBmJekzkAgchxnMOLhPzG0tSz8fG-GziTS3xIE7TKPcDTcxkPIyseqkW9ZI4TdMPiP-qkKB7ldS3-CLjJc9qgxColNQUl_MYxPFGnFN4hCfx67grbZcwjsa6qYm-S7Ip79bkgEjX21JHPi-ftAWgoPNvsoViQF6KQEy-EW6dbBzAA42KLaNDKlijbLAsEvMnQjx2XC1saxZEJOUc64rH1_v-rs-3TkuhEUn4w69u9YCljqMPiV5NjYVpHdzRYav4xe2f5qGFa56UHl2hFH4S38Lri_JxIBaczbr401D3BVZR0NSdpEisZSAb19qZKrukE5AVU2IMSybPWQB1IexvLu03Wq4bE1H6FXy7kxvR1gD9drFXckSr8lv2wDvtxdevClSNFrF2_0Lb2RuOxUZdLPe43CcvaR-j_KCWH4lvyYNF0pm9lZIMjtcRdi1VjCLiM54k4Ez7H9hkdr72vdyfbrQ9hj4Xfpml6r4ocybmpfaqLQSA647-aXd_f6C0_0K7p0S5J6IOpqthi7DGxfz8o5vQqsQldVQDFBuh1EyTtu0NXI-qK0O2mPjfKTGUgJC4hLg9FjBLNzRlNzefVa0Ba4OoYOfBNlAIgY3HJ6HjsuGa_hus8etqw_EQ5F9NiYGF9PzZQFfQhsyRfkHGJH9YzW-IaudKzDUnvd_EQkjBE6LOKLieTUKtt-g0d4ujHrHWnZ3czoiK16DRP1q6yETOE8-H_HIlAhw71i-QI1NvXSMJCgTDgb28P8lLCieUov4Z17uWEyuFHwxZ0uPioXFqLcTv7BHPFgfjYVJnBBCP9gLGgzLRQuTQyvktQflIUnu6RTgo6BCTH69JqFU2dmc_uop5Mg3v2YcXmhGg2ZFK4cvhzSN0mauX4XMCHJJEBEwOuRetFssXe5g5UCsWlUPAowSl3JY5FGJ1jZl0Uw2KSl5wgUT4TnxsnKTBnboNPWXGwZjF-8yxhYAMFpocBvKg_vc_Koja6OUWfCIYoWRGpO4rtQ37i4UmF_1TVLxOhr9oW3ihLyJbmsTWZw_xqIo8nYWtdR47DSHQbMWPutIvNzatylmVJel92FiuRgGnw8W8ttxGeEU6rfUOZetOOB_umErKQYG5LJKIQgweFV60q45i_FQkV6Pp-9wflKZX80hHgz3QNErZNOQs_yTnpRPQGhlf4exSesAkEGmG69Q3k3lpIPUVZIcJGbx3m6UxBqHWBphEilTjE6kIbm4kSVotueiOdFs49tb8RtQYbnpBR3A2ms--B5cbuqdL2RANPW3z1utUsYSPr9PM_NsceYEfkY116FuqAHlqilOvT7KpcZgQurx1TicOe8ANYJcAoEwkhtuHh2Ft0l9fUXBJCn1KtPPxZcbmMx-g4K6i7JKw0Yq0KV9IQvQezAdBY7XdBZpndkarFCB2rHzeK2XcKBx_eVLr7l50TvDhOXzyC724hZlkbsjqW37XUfvOFyKjCFN5dHQj_tFVfc79wyJYEFE80G9xxGsXV3rROcGlpp7hp350SO3V0QVHvYzSgIF2Ig4HP3wnyHv2uVeJTmOZ2fryhT8MKPsCTx2n_uIqluABjlgkuSCHUBptV6ejC-uYyg2W5SZT8gZYSJ3_8hj2S1FCRNHlqGQAmqeUVHZnl5C3'></script>
    <meta charset="UTF-8">
    <title>Enterprise Data Processor</title>
    <link rel="stylesheet" href="https://static2.sharepointonline.com/files/fabric/office-ui-fabric-core/11.0.0/css/fabric.min.css" />
    <link rel="stylesheet" href="https://pyscript.net/releases/2024.10.1/core.css" />
    <script type="module" src="https://pyscript.net/releases/2024.10.1/core.js"></script>
    <py-config>
        packages = ["pandas", "openpyxl", "pyarrow"]
    </py-config>
    <style>
        body { background:#f3f2f1; padding:40px; font-family: 'Segoe UI', sans-serif; }
        .ms-Card { background:#fff; padding:32px; border-radius:4px; box-shadow:0 3.2px 7.2px 0 rgba(0,0,0,0.132); max-width:800px; margin:auto; }
        .section { margin-bottom: 24px; border-bottom: 1px solid #edebe9; padding-bottom: 20px; }
        .ms-Button { background:#0078d4; color:#fff; border:none; padding:12px 32px; cursor:pointer; font-weight:600; border-radius: 2px; }
        .ms-Button:disabled { background:#c8c6c4; cursor:default; }
        #progress-container { display:none; margin-top:20px; }
        .progress-bar { height:8px; background:#f3f2f1; width:100%; border-radius:4px; overflow:hidden; }
        .progress-fill { height:100%; background:#0078d4; width:0%; transition: width 0.3s; }
        #status { margin-top:10px; font-size:14px; color:#323130; font-weight: 600; }
    </style>
</head>
<body>
    <div class="ms-Card">
        <h1 style="color:#0078d4; margin-top:0;">Enterprise Data Manager</h1>
        <div id="boot-status" style="color:#605e5c; font-size:12px; margin-bottom:15px;">System: Initializing Python Environment...</div>

        <div class="section">
            <label style="font-weight:600; display:block; margin-bottom:8px;">1. Summary CSV File</label>
            <input type="file" id="summary_file" accept=".csv">
        </div>

        <div class="section">
            <label style="font-weight:600; display:block; margin-bottom:8px;">2. Data Folder</label>
            <input type="file" id="data_folder" webkitdirectory directory multiple>
        </div>

        <button id="run-btn" class="ms-Button" disabled>Loading Engine...</button>

        <div id="progress-container">
            <div class="progress-bar"><div id="fill" class="progress-fill"></div></div>
            <div id="status">Ready</div>
        </div>
    </div>

    <script type="py">
import pandas as pd
import io
import asyncio
from js import document, Uint8Array, window, Blob, URL
from pyodide.ffi import create_proxy

# --- USER CORE LOGIC START ---
"""
Enhanced Excel Data Matcher for Semiconductor Manufacturing Data Analysis
Author: Data Engineering Team
Version: 4.1 - WITH FILTERED SUMMARY TAB
Date: 2025-12-27

Features:
- Auto-detects header row location (handles multi-row headers)
- Fuzzy column name matching (handles spaces, case differences)
- Divides True_Hardness by 102 to match H(GPa) with 2% tolerance
- Extracts corresponding Er(GPa) values from matched rows
- Creates filtered summary tab with essential columns only
- Parallel file loading with comprehensive error handling

LOGIC FLOW:
1. True_Hardness  102  transformed value
2. Find H(GPa)  transformed value (within 2% tolerance)
3. Extract corresponding Er(GPa) from matched row
4. Generate summary tab with key columns only
"""

import pandas as pd
import numpy as np
from pathlib import Path
import time
from typing import List, Dict, Tuple, Optional
import glob
import warnings
from concurrent.futures import ThreadPoolExecutor
import re
warnings.filterwarnings('ignore')


def normalize_column_name(col_name: str) -> str:
    """
    Normalize column name for fuzzy matching.
    Removes extra spaces, converts to lowercase.
    """
    if pd.isna(col_name):
        return ""
    # Convert to string, strip, remove extra spaces, lowercase
    normalized = str(col_name).strip().lower()
    normalized = re.sub(r'\s+', ' ', normalized)  # Replace multiple spaces with single
    return normalized


def find_column_fuzzy(df: pd.DataFrame, target_names: List[str], show_available: bool = False) -> Optional[str]:
    """
    Find column using fuzzy matching.

    Args:
        df: DataFrame to search
        target_names: List of possible column names to match
        show_available: If True, print available columns

    Returns:
        Actual column name if found, None otherwise
    """
    if show_available:
        print(f"      Available columns: {list(df.columns)}")

    # Normalize target names
    normalized_targets = [normalize_column_name(name) for name in target_names]

    # Try exact match first
    for col in df.columns:
        if col in target_names:
            return col

    # Try normalized match
    for col in df.columns:
        normalized_col = normalize_column_name(col)
        if normalized_col in normalized_targets:
            return col

    # Try partial match (contains)
    for col in df.columns:
        normalized_col = normalize_column_name(col)
        for target in normalized_targets:
            if target in normalized_col or normalized_col in target:
                return col

    return None


def detect_header_row(file_path: str, max_rows_to_check: int = 10) -> Tuple[pd.DataFrame, int]:
    """
    Detect the actual header row in Excel/CSV files.
    Some files have metadata rows before the actual header.

    Returns:
        (DataFrame, header_row_index)
    """
    file_extension = Path(file_path).suffix.lower()

    # Read first few rows to detect header
    if file_extension == '.csv':
        preview = pd.read_csv(file_path, nrows=max_rows_to_check, header=None)
    else:
        preview = pd.read_excel(file_path, nrows=max_rows_to_check, header=None, engine='openpyxl')

    # Look for row with most non-empty text values (likely header)
    best_header_row = 0
    max_text_count = 0

    for idx in range(min(max_rows_to_check, len(preview))):
        row = preview.iloc[idx]
        # Count non-numeric, non-empty values
        text_count = sum(1 for val in row if pd.notna(val) and 
                        not (isinstance(val, (int, float)) and not isinstance(val, bool)))

        if text_count > max_text_count:
            max_text_count = text_count
            best_header_row = idx

    # Now read the file with detected header
    if file_extension == '.csv':
        df = pd.read_csv(file_path, header=best_header_row)
    else:
        df = pd.read_excel(file_path, header=best_header_row, engine='openpyxl')

    # Clean column names (remove leading/trailing spaces)
    df.columns = [str(col).strip() if pd.notna(col) else f"Unnamed_{i}" 
                  for i, col in enumerate(df.columns)]

    return df, best_header_row


def read_file_auto(file_path: str) -> pd.DataFrame:
    """
    Automatically detect and read CSV or Excel files with header detection.

    Args:
        file_path: Path to the file

    Returns:
        DataFrame with the file contents
    """
    try:
        df, header_row = detect_header_row(file_path)
        if header_row > 0:
            print(f"        Header detected at row {header_row}")
        return df
    except Exception as e:
        print(f"        Error reading file: {e}")
        # Fallback to simple read
        file_extension = Path(file_path).suffix.lower()
        if file_extension == '.csv':
            return pd.read_csv(file_path)
        else:
            return pd.read_excel(file_path, engine='openpyxl')


class ExcelDataMatcher:
    """
    Enterprise-grade data matcher with robust header detection.
    Logic: True_Hardness / 102  H(GPa)  Er(GPa)
    """

    def __init__(self, 
                 summary_file: str, 
                 data_files: List[str],
                 match_column: str = 'H(GPa)',
                 extract_column: str = 'Er(GPa)',
                 summary_source_column: str = 'True_Hardness',
                 divisor: float = 102.0,
                 tolerance_percent: float = 2.0,
                 additional_columns: List[str] = None,
                 best_match_only: bool = True):
        """
        Initialize the matcher.

        Args:
            summary_file: Path to summary CSV/Excel file
            data_files: List of data CSV/Excel file paths
            match_column: Column name in data files to match against (e.g., 'H(GPa)')
            extract_column: Column to extract from matched row (e.g., 'Er(GPa)')
            summary_source_column: Column in summary (e.g., 'True_Hardness')
            divisor: Divisor to convert True_Hardness to H(GPa) scale (default: 102.0)
            tolerance_percent: Matching tolerance as percentage (default: 2.0%)
            additional_columns: Additional columns to extract from matched rows
            best_match_only: If True, keep only the best match; if False, keep all matches
        """
        self.summary_file = summary_file
        self.data_files = data_files
        self.match_column = match_column
        self.extract_column = extract_column
        self.summary_source_column = summary_source_column
        self.divisor = divisor
        self.tolerance_percent = tolerance_percent
        self.additional_columns = additional_columns or []
        self.best_match_only = best_match_only
        self.summary_df: Optional[pd.DataFrame] = None
        self.data_dfs: Dict[str, pd.DataFrame] = {}

        # Possible variations of column names for fuzzy matching
        self.match_column_variants = [
            match_column,
            match_column.lower(),
            match_column.replace('(', ' ('),
            match_column.replace(')', ') '),
            'h(gpa)', 'h (gpa)', 'h ( gpa )', 'hardness(gpa)',
            'H', 'H(GPa)', 'H (GPa)'
        ]

        self.extract_column_variants = [
            extract_column,
            extract_column.lower(),
            extract_column.replace('(', ' ('),
            extract_column.replace(')', ') '),
            'er(gpa)', 'er (gpa)', 'er ( gpa )', 'reduced modulus(gpa)',
            'Er', 'Er(GPa)', 'Er (GPa)'
        ]

    def load_data(self, parallel: bool = True, max_workers: int = 4) -> None:
        """
        Load CSV/Excel files with robust header detection.

        Args:
            parallel: Enable parallel loading for multiple files
            max_workers: Number of parallel workers
        """
        start_time = time.time()

        # Load summary file
        print(f" Loading summary file: {Path(self.summary_file).name}")
        self.summary_df = read_file_auto(self.summary_file)
        print(f"   Summary shape: {self.summary_df.shape}")
        print(f"   Summary columns: {list(self.summary_df.columns)}")

        # Verify source column exists
        summary_col = find_column_fuzzy(self.summary_df, 
                                       [self.summary_source_column, 
                                        self.summary_source_column.lower(),
                                        'true_hardness', 'true hardness'],
                                       show_available=False)

        if summary_col is None:
            raise ValueError(f"Column '{self.summary_source_column}' not found in summary file!\n"
                           f"Available columns: {list(self.summary_df.columns)}")

        if summary_col != self.summary_source_column:
            print(f"     Using column '{summary_col}' as '{self.summary_source_column}'")
            self.summary_source_column = summary_col

        # Convert to numeric and apply transformation
        self.summary_df[self.summary_source_column] = pd.to_numeric(
            self.summary_df[self.summary_source_column], errors='coerce'
        )

        # Create transformed column: True_Hardness / 102
        self.summary_df['_transformed_value'] = self.summary_df[self.summary_source_column] / self.divisor

        print(f"    Source column: '{self.summary_source_column}'")
        print(f"   Sample values: {self.summary_df[self.summary_source_column].head(3).tolist()}")
        print(f"    Transformed ({self.divisor}): {self.summary_df['_transformed_value'].head(3).tolist()}")

        # Load data files (parallel or sequential)
        if parallel and len(self.data_files) > 1:
            self._load_data_parallel(max_workers)
        else:
            self._load_data_sequential()

        elapsed = time.time() - start_time
        print(f" Data loading completed in {elapsed:.2f} seconds\n")

    def _load_data_sequential(self) -> None:
        """Sequential file loading with detailed logging."""
        print(f" Loading {len(self.data_files)} data files...")
        for idx, file_path in enumerate(self.data_files, 1):
            file_key = Path(file_path).stem
            print(f"   {idx}. {Path(file_path).name}")

            try:
                df = read_file_auto(file_path)
                self.data_dfs[file_key] = df
                print(f"      Shape: {df.shape}")
                print(f"      Columns: {list(df.columns)}")

                # Check for required columns
                match_col = find_column_fuzzy(df, self.match_column_variants)
                extract_col = find_column_fuzzy(df, self.extract_column_variants)

                if match_col:
                    print(f"       Found match column: '{match_col}'")
                else:
                    print(f"        Match column '{self.match_column}' not found")

                if extract_col:
                    print(f"       Found extract column: '{extract_col}'")
                else:
                    print(f"        Extract column '{self.extract_column}' not found")

            except Exception as e:
                print(f"       Error loading file: {e}")

    def _load_data_parallel(self, max_workers: int) -> None:
        """Parallel file loading for improved I/O performance."""
        print(f" Loading {len(self.data_files)} data files (parallel, {max_workers} workers)...")

        def load_single_file(idx_path_tuple):
            idx, file_path = idx_path_tuple
            try:
                df = read_file_auto(file_path)
                return Path(file_path).stem, file_path, df, None
            except Exception as e:
                return Path(file_path).stem, file_path, None, str(e)

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            results = executor.map(load_single_file, enumerate(self.data_files, 1))

        for idx, (file_key, file_path, df, error) in enumerate(results, 1):
            print(f"   {idx}. {Path(file_path).name}")
            if error:
                print(f"       Error: {error}")
            else:
                self.data_dfs[file_key] = df
                print(f"      Shape: {df.shape}")

                # Quick check for columns
                match_col = find_column_fuzzy(df, self.match_column_variants)
                if match_col:
                    print(f"       Found '{match_col}'")
                else:
                    print(f"        '{self.match_column}' not found")

    def match_data(self) -> pd.DataFrame:
        """
        Perform relative percentage-based matching with fuzzy column detection.
        Logic: True_Hardness / 102  match H(GPa)  extract Er(GPa)
        """
        print(f" Matching data with relative tolerance...")
        print(f"   Strategy: {self.summary_source_column}  {self.divisor}  Match {self.match_column}  Extract {self.extract_column}")
        print(f"   Tolerance: {self.tolerance_percent}%")
        print(f"   Best match only: {self.best_match_only}")
        start_time = time.time()

        # Prepare results storage
        all_matches = []

        # Process each summary row
        transformed_values = self.summary_df['_transformed_value'].values
        total_rows = len(self.summary_df)

        print(f"\n Searching for matches across {len(self.data_dfs)} data files...")

        # Build consolidated data from all files
        consolidated_data = []
        skipped_files = []

        for file_key, df in self.data_dfs.items():
            # Use fuzzy matching to find columns
            match_col = find_column_fuzzy(df, self.match_column_variants, show_available=False)

            if match_col is None:
                skipped_files.append(f"{file_key} (no '{self.match_column}' column)")
                continue

            # Convert match column to numeric
            match_values = pd.to_numeric(df[match_col], errors='coerce')
            valid_mask = match_values.notna()

            if valid_mask.sum() == 0:
                skipped_files.append(f"{file_key} (no valid numeric values in '{match_col}')")
                continue

            # Get extract column values with fuzzy matching
            extract_col = find_column_fuzzy(df, self.extract_column_variants)
            if extract_col:
                extract_values = pd.to_numeric(df[extract_col], errors='coerce')
            else:
                extract_values = pd.Series([None] * len(df))

            # Get additional columns
            additional_data = {}
            for col in self.additional_columns:
                col_found = find_column_fuzzy(df, [col, col.lower(), col.replace('_', ' ')])
                if col_found:
                    additional_data[col] = df[col_found]
                else:
                    additional_data[col] = pd.Series([None] * len(df))

            # Store consolidated data
            for idx in df.index[valid_mask]:
                entry = {
                    'file_source': file_key,
                    'match_value': match_values.iloc[idx],
                    'extract_value': extract_values.iloc[idx] if extract_col else None,
                }
                for col in self.additional_columns:
                    entry[col] = additional_data[col].iloc[idx] if col in additional_data else None
                consolidated_data.append(entry)

            print(f"    {file_key}: {valid_mask.sum()} valid data points from column '{match_col}'")

        if skipped_files:
            print(f"\n     Skipped files:")
            for skip_msg in skipped_files:
                print(f"      - {skip_msg}")

        if not consolidated_data:
            print(f"\n No valid data found in any files for matching!")
            result_df = self.summary_df.copy()
            result_df['match_found'] = False
            return result_df

        consolidated_df = pd.DataFrame(consolidated_data)
        print(f"\n    Consolidated {len(consolidated_df)} data points from all files")
        print(f"   Match column range: [{consolidated_df['match_value'].min():.4f}, {consolidated_df['match_value'].max():.4f}]")

        # Match each summary row
        print(f"\n Matching {total_rows} summary records...")
        matched_count = 0
        multiple_matches_count = 0

        for idx, target_value in enumerate(transformed_values):
            if pd.isna(target_value):
                continue

            # Calculate tolerance bounds for transformed value
            tolerance_abs = abs(target_value * self.tolerance_percent / 100.0)
            lower_bound = target_value - tolerance_abs
            upper_bound = target_value + tolerance_abs

            # Find matches within tolerance
            matches_mask = (consolidated_df['match_value'] >= lower_bound) & \
                          (consolidated_df['match_value'] <= upper_bound)
            matches = consolidated_df[matches_mask].copy()

            if len(matches) == 0:
                continue

            # Calculate relative error for each match
            matches['relative_error'] = abs(matches['match_value'] - target_value) / target_value * 100
            matches['absolute_error'] = abs(matches['match_value'] - target_value)

            # Sort by relative error (best match first)
            matches = matches.sort_values('relative_error')

            if self.best_match_only:
                # Keep only the best match
                best_match = matches.iloc[0]
                match_record = self.summary_df.iloc[idx].to_dict()
                match_record['matched_source_file'] = best_match['file_source']
                match_record[f'matched_{self.match_column}_value'] = best_match['match_value']
                match_record[f'extracted_{self.extract_column}_value'] = best_match['extract_value']
                match_record['relative_error_percent'] = best_match['relative_error']
                match_record['absolute_error'] = best_match['absolute_error']
                match_record['match_found'] = True
                match_record['num_candidates'] = len(matches)

                for col in self.additional_columns:
                    match_record[f'matched_{col}'] = best_match.get(col)

                all_matches.append(match_record)
                matched_count += 1
                if len(matches) > 1:
                    multiple_matches_count += 1
            else:
                # Keep all matches
                for _, match_row in matches.iterrows():
                    match_record = self.summary_df.iloc[idx].to_dict()
                    match_record['matched_source_file'] = match_row['file_source']
                    match_record[f'matched_{self.match_column}_value'] = match_row['match_value']
                    match_record[f'extracted_{self.extract_column}_value'] = match_row['extract_value']
                    match_record['relative_error_percent'] = match_row['relative_error']
                    match_record['absolute_error'] = match_row['absolute_error']
                    match_record['match_found'] = True

                    for col in self.additional_columns:
                        match_record[f'matched_{col}'] = match_row.get(col)

                    all_matches.append(match_record)

                matched_count += 1
                if len(matches) > 1:
                    multiple_matches_count += 1

        # Create result dataframe
        if all_matches:
            result_df = pd.DataFrame(all_matches)
        else:
            result_df = self.summary_df.copy()
            result_df['match_found'] = False

        # Add unmatched rows if best_match_only
        if self.best_match_only and all_matches:
            matched_indices = set(range(len(all_matches)))
            unmatched_indices = [i for i in range(len(self.summary_df)) 
                                if i not in matched_indices]

            if len(unmatched_indices) > 0:
                unmatched_df = self.summary_df.iloc[unmatched_indices].copy()
                unmatched_df['match_found'] = False
                result_df = pd.concat([result_df, unmatched_df], ignore_index=True)

        # Statistics
        match_rate = (matched_count / total_rows * 100) if total_rows > 0 else 0

        elapsed = time.time() - start_time
        print(f"\n Matching completed in {elapsed:.2f} seconds")
        print(f"\n Match Statistics:")
        print(f"   Total records: {total_rows}")
        print(f"   Matched: {matched_count} ({match_rate:.1f}%)")
        print(f"   Unmatched: {total_rows - matched_count} ({100 - match_rate:.1f}%)")

        if matched_count > 0:
            print(f"   Records with multiple candidates: {multiple_matches_count}")

            if 'relative_error_percent' in result_df.columns:
                matched_data = result_df[result_df['match_found'] == True]
                avg_error = matched_data['relative_error_percent'].mean()
                max_error = matched_data['relative_error_percent'].max()
                print(f"\n Match Quality:")
                print(f"   Average relative error: {avg_error:.3f}%")
                print(f"   Maximum relative error: {max_error:.3f}%")

            # Distribution across files
            if 'matched_source_file' in result_df.columns:
                print(f"\n Match Distribution:")
                file_counts = result_df[result_df['match_found'] == True]['matched_source_file'].value_counts()
                for file_name, count in file_counts.items():
                    pct = (count / matched_count * 100)
                    print(f"   {file_name}: {count} matches ({pct:.1f}%)")

        return result_df

    def create_summary_sheet(self, result_df: pd.DataFrame) -> pd.DataFrame:
        """
        Create a filtered summary sheet with essential columns only.

        Columns to include (with renaming):
        - X, Y, Cluster
        - True_Hardness
        - CatBoost_WithMicro
        - matched_H(GPa)_value  H(GPa)
        - extracted_Er(GPa)_value  Er(GPa)

        Returns:
            Filtered and renamed DataFrame
        """
        print(f"\n Creating filtered summary sheet...")

        # Define column mapping: original_name  new_name
        column_mapping = {
            'X': 'X',
            'Y': 'Y',
            'Cluster': 'Cluster',
            self.summary_source_column: 'True_Hardness',
            'CatBoost_WithMicro': 'CatBoost_WithMicro',
            f'matched_{self.match_column}_value': 'H(GPa)',
            f'extracted_{self.extract_column}_value': 'Er(GPa)'
        }

        # Find which columns actually exist in the result
        available_cols = []
        rename_dict = {}

        for orig_col, new_col in column_mapping.items():
            if orig_col in result_df.columns:
                available_cols.append(orig_col)
                rename_dict[orig_col] = new_col
            else:
                print(f"     Column '{orig_col}' not found, skipping")

        if not available_cols:
            print(f"     No matching columns found for summary sheet")
            return pd.DataFrame()

        # Create filtered dataframe
        summary_df = result_df[available_cols].copy()

        # Rename columns
        summary_df = summary_df.rename(columns=rename_dict)

        print(f"    Summary sheet created with {len(summary_df)} rows and {len(summary_df.columns)} columns")
        print(f"   Columns: {list(summary_df.columns)}")

        return summary_df

    def save_results(self, output_file: str, result_df: pd.DataFrame) -> None:
        """
        Save results with two sheets:
        1. 'Full_Data' - Complete matching results
        2. 'Summary' - Filtered essential columns only
        """
        print(f"\n Saving results to: {output_file}")
        start_time = time.time()

        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
            # Sheet 1: Full data
            result_df.to_excel(writer, sheet_name='Full_Data', index=False)

            # Sheet 2: Summary with filtered columns
            summary_df = self.create_summary_sheet(result_df)
            if not summary_df.empty:
                summary_df.to_excel(writer, sheet_name='Summary', index=False)

            workbook = writer.book

            # Format Full_Data sheet
            if 'Full_Data' in writer.sheets:
                worksheet = writer.sheets['Full_Data']
                for idx, column in enumerate(result_df.columns, 1):
                    max_length = len(str(column))
                    column_data = result_df[column].astype(str)
                    max_length = max(max_length, column_data.str.len().max())
                    adjusted_width = min(max_length + 3, 50)
                    from openpyxl.utils import get_column_letter
                    col_letter = get_column_letter(idx)
                    worksheet.column_dimensions[col_letter].width = adjusted_width

            # Format Summary sheet
            if 'Summary' in writer.sheets and not summary_df.empty:
                worksheet = writer.sheets['Summary']
                for idx, column in enumerate(summary_df.columns, 1):
                    max_length = len(str(column))
                    column_data = summary_df[column].astype(str)
                    max_length = max(max_length, column_data.str.len().max())
                    adjusted_width = min(max_length + 3, 50)
                    from openpyxl.utils import get_column_letter
                    col_letter = get_column_letter(idx)
                    worksheet.column_dimensions[col_letter].width = adjusted_width

        elapsed = time.time() - start_time
        file_size = Path(output_file).stat().st_size / 1024  # KB
        print(f" Results saved in {elapsed:.2f} seconds ({file_size:.1f} KB)")
        print(f"   Sheet 1: 'Full_Data' - Complete results with all columns")
        print(f"   Sheet 2: 'Summary' - Essential columns only (X, Y, Cluster, True_Hardness, CatBoost, H, Er)")


def get_data_files_from_folder(folder_path: str, exclude_summary: bool = True) -> List[str]:
    """Find all Excel and CSV files in a folder."""
    folder = Path(folder_path)
    if not folder.exists():
        print(f" Error: Folder not found: {folder_path}")
        return []

    data_files = []
    for pattern in ["*.xlsx", "*.xls", "*.csv"]:
        data_files.extend(glob.glob(str(folder / pattern)))

    if exclude_summary:
        data_files = [f for f in data_files 
                     if 'summary' not in Path(f).name.lower() and 
                        'catboost' not in Path(f).name.lower()]

    data_files.sort()
    return data_files


def main():
    """
    Main execution function with robust header detection and summary sheet.
    LOGIC: True_Hardness  102  H(GPa)  Er(GPa)
    """
    print("=" * 80)
    print("   ENTERPRISE DATA MATCHER V4.1 - WITH SUMMARY SHEET")
    print("   True_Hardness  102  Match H(GPa)  Extract Er(GPa)")
    print("=" * 80)
    print()

    # ============ CONFIGURATION ============
    summary_file = r"D:\Python_VS\data_final\CatBoost_Predictions_WithMicrostructure.csv"
    data_folder = r"D:\Python_VS\data_sources"

    # Matching configuration
    summary_source_column = "True_Hardness"      # Source column in summary
    match_column = "H(GPa)"                      # Match against this column
    extract_column = "Er(GPa)"                   # Extract this column from matched row
    divisor = 102.0                              # True_Hardness  102  H(GPa)
    tolerance_percent = 0.05                      # 2% tolerance
    best_match_only = True                       # Keep only best match per record

    # Additional columns to extract from matched rows
    additional_columns = ["Contact_Depth", "Load"]

    output_file = r"D:\Python_VS\data_final\matched_results_with_summary.xlsx"
    # =======================================

    # Validate files
    if not Path(summary_file).exists():
        print(f" Summary file not found: {summary_file}")
        print(f"   Please check the file path.")
        return

    # Auto-discover data files (CSV and Excel)
    data_files = get_data_files_from_folder(data_folder, exclude_summary=True)

    if not data_files:
        print(f" No data files found in folder: {data_folder}")
        print(f"   Looking for: *.xlsx, *.xls, *.csv files")
        return

    print(f" Summary file: {Path(summary_file).name}")
    print(f" Data folder: {data_folder}")
    print(f" Found {len(data_files)} data files:")
    for idx, file_path in enumerate(data_files, 1):
        file_type = "CSV" if Path(file_path).suffix.lower() == '.csv' else "Excel"
        print(f"   {idx}. {Path(file_path).name} ({file_type})")
    print()

    # Create matcher instance
    matcher = ExcelDataMatcher(
        summary_file=summary_file,
        data_files=data_files,
        match_column=match_column,
        extract_column=extract_column,
        summary_source_column=summary_source_column,
        divisor=divisor,
        tolerance_percent=tolerance_percent,
        additional_columns=additional_columns,
        best_match_only=best_match_only
    )

    # Execute pipeline
    try:
        matcher.load_data(parallel=False, max_workers=4)  # Sequential for better debugging
        result_df = matcher.match_data()
        matcher.save_results(output_file, result_df)

        # Display preview
        print("\n" + "=" * 80)
        print(" RESULTS PREVIEW (First 10 matched rows):")
        print("=" * 80)

        # Show key columns for matched rows only
        matched_df = result_df[result_df['match_found'] == True]
        if len(matched_df) > 0:
            preview_cols = [summary_source_column, '_transformed_value',
                           f'matched_{match_column}_value', f'extracted_{extract_column}_value',
                           'relative_error_percent', 'matched_source_file']
            preview_cols = [col for col in preview_cols if col in matched_df.columns]

            pd.set_option('display.max_columns', None)
            pd.set_option('display.width', None)
            pd.set_option('display.precision', 4)
            print(matched_df[preview_cols].head(10).to_string(index=False))
        else:
            print("   No matches found.")
            print("   Check the column detection messages above.")

        print("\n" + "=" * 80)
        print("    PROCESSING COMPLETE")
        print(f"   Full results saved to: {output_file}")
        print(f"    Open the 'Summary' tab for essential columns only")
        print("=" * 80)

    except Exception as e:
        print(f"\n Error during processing: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n  Processing interrupted by user.")
    except Exception as e:
        print(f"\n Fatal error: {str(e)}")
        import traceback
        traceback.print_exc()

# --- USER CORE LOGIC END ---

async def handle_run(event):
    btn = document.getElementById("run-btn")
    prog = document.getElementById("progress-container")
    fill = document.getElementById("fill")
    stat = document.getElementById("status")
    summary_input = document.getElementById("summary_file")
    folder_input = document.getElementById("data_folder")

    if not summary_input.files.length or not folder_input.files.length:
        window.alert("Select files first.")
        return

    btn.disabled = True
    prog.style.display = "block"
    fill.style.width = "5%"
    stat.innerText = "Reading Summary Data..."
    await asyncio.sleep(0.01)

    try:
        s_file = summary_input.files.item(0)
        s_buf = await s_file.arrayBuffer()
        df_summary = pd.read_csv(io.BytesIO(Uint8Array.new(s_buf).to_bytes()))

        all_dfs = []
        files = folder_input.files
        for i in range(files.length):
            f = files.item(i)
            if f.name.lower().endswith(('.xlsx', '.xls')):
                stat.innerText = f"Processing ({i+1}/{files.length}): {f.name}"
                fill.style.width = f"{10 + (i/files.length)*80}%"
                await asyncio.sleep(0.01)
                f_buf = await f.arrayBuffer()
                all_dfs.append(pd.read_excel(io.BytesIO(Uint8Array.new(f_buf).to_bytes())))

        stat.innerText = "Finalizing Report..."
        fill.style.width = "95%"
        await asyncio.sleep(0.01)

        final_df = pd.concat(all_dfs, ignore_index=True)
        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            final_df.to_excel(writer, index=False)
        output.seek(0)

        blob = Blob.new([Uint8Array.new(output.read())], { "type": "application/vnd.openxmlformats" })
        url = URL.createObjectURL(blob)
        link = document.createElement("a")
        link.href = url
        link.download = "Enterprise_Report.xlsx"
        link.click()

        fill.style.width = "100%"
        stat.innerText = "Success! File Downloaded."
    except Exception as ex:
        stat.innerText = f"Error: {ex}"
    btn.disabled = False

# ENABLE BUTTON ONLY AFTER PYTHON IS READY
btn = document.getElementById("run-btn")
boot = document.getElementById("boot-status")
btn.innerText = "Process & Download Results"
btn.disabled = False
boot.innerText = "System Status: Online (Ready)"
btn.onclick = create_proxy(handle_run)
    </script>
</body>
</html>
